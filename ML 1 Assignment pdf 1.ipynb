{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What are the three stages to build the hypotheses or model in machine learning? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three stages to build the hypothesis or model in machine learning are below:\n",
    "\n",
    "    a) Model building\n",
    "    b) Model testing \n",
    "    c) Applying the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  What is the standard approach to supervised learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard approach to supervised learning is to split the data into tarin and test dataset and doing the modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What is Training set and Test set? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set:\n",
    "\n",
    "    The training data set in Machine Learning is the actual dataset used to train the model for performing various actions. \n",
    "    This is the actual data the ongoing development process models learn with various API and algorithm to train the machine to work automatically. Training set is an examples given to the learner and Test set is used to test the accuracy of the hypotheses generated by the learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set:\n",
    "\n",
    "    The test set is a dataset used to measure how well the model performs at making predictions on that test set.  For example, in the case of sentiment analysis, a test set is a dataset of tweets that are distinct from the tweets in the training set. If the prediction scores (sentiment scores) for the test set are unreasonable, we’ll need to make some adjustments to our model and try again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.What is the general principle of an ensemble method and what is bagging and boosting in ensemble method? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general principle of an ensemble method is to combine the predictions of several models built with a given learning algorithm in order to improve robustness over a single model. bagging is a method in ensemble for improving unstable estimation or classification schemes.\n",
    "Ensemble methods are meta-algorithms that combine several machine learning techniques into one predictive model in order to decrease variance (bagging), bias (boosting), or improve predictions (stacking).\n",
    "#### Bagging:\n",
    "Bagging (Bootstrap Aggregating)  is an ensemble method. First, we create random samples of the training data set (sub sets of training data set). Then, we build a classifier for each sample. Finally, results of these multiple classifiers are combined using average or majority voting. Bagging helps to reduce the variance error.\n",
    "\n",
    "#### Boosting:\n",
    "Boosting provides sequential learning of the predictors. The first predictor is learned on the whole data set, while the following are learnt on the training set based on the performance of the previous one. It starts by classifying original data set and giving equal weights to each observation. If classes are predicted incorrectly using the first learner, then it gives higher weight to the missed classified observation. Being an iterative process, it continues to add classifier learner until a limit is reached in the number of models or accuracy. Boosting has shown better predictive accuracy than bagging, but it also tends to over-fit the training data as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  How can you avoid overfitting ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting\n",
    "Overfitting refers to a model that models the training data too well.\n",
    "\n",
    "Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model. The problem is that these concepts do not apply to new data and negatively impact the models ability to generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few of the most popular solutions for overfitting:\n",
    "\n",
    "#### 1. Cross-validation\n",
    "Cross-validation is a powerful preventative measure against overfitting.\n",
    "\n",
    "Use initial training data to generate multiple mini train-test splits. Use these splits to tune your model.\n",
    "\n",
    "In standard k-fold cross-validation, we partition the data into k subsets, called folds. Then, we iteratively train the algorithm on k-1 folds while using the remaining fold as the test set (called the holdout fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Train with more data\n",
    "It won’t work every time, but training with more data can help algorithms detect the signal better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Remove features\n",
    "W ecan remove the features which are irrelevant and improve the model. Some algorithms have built-in feature selection.\n",
    "For those that don’t, we can manually improve their generalizability by removing the unwanted features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Early stopping\n",
    "When we are training a learning algorithm iteratively, we can measure how well each iteration of the model performs. Upto certain number of iterations, new iterations improve the model. After that point, however, the model’s ability to generalize can weaken as it begins to overfit the training data.\n",
    "\n",
    "Early stopping refers stopping the training process before the learner passes that point. In this way we can avoid overfitting by early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Ensembling\n",
    "Ensembles are machine learning methods for combining predictions from multiple separate models. This technique helps in avoidin gthe overfitting. The two most common methods are:\n",
    "\n",
    "1. Bagging: It attempts to reduce the chance overfitting complex models.\n",
    "2. Boosting: It attempts to improve the predictive flexibility of simple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
